{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyse des Temp√©ratures Journali√®res et Anomalies Thermiques en France\n",
        "\n",
        "L'objectif de cette analyse est de pr√©parer les donn√©es m√©t√©orologiques journali√®res pour √©tudier les temp√©ratures maximales journali√®res en France, en calculant la moyenne glissante sur 30 jours et en d√©terminant les anomalies thermiques. Les donn√©es proviennent du fichier `Q_95_latest-2024-2025_RR-T-Vent.csv.gz` disponible sur le site [data.gouv.fr](https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/QUOT/Q_95_latest-2024-2025_RR-T-Vent.csv.gz), qui contient des informations m√©t√©orologiques quotidiennes pour diverses stations en France.\n",
        "\n",
        "Ce fichier de donn√©es est pr√©cieux pour comprendre les variations climatiques et m√©t√©orologiques en France. L'analyse se concentre sur la temp√©rature moyenne journali√®re (`TM`), le calcul de la moyenne glissante sur 30 jours, et l'identification des anomalies en comparant les valeurs journali√®res avec cette moyenne glissante. Ensuite, ces anomalies sont liss√©es sur 7 jours pour une meilleure visualisation.\n",
        "\n",
        "\n",
        "## M√©thodologie\n",
        "\n",
        "La m√©thodologie utilis√©e implique plusieurs √©tapes cl√©s :\n",
        "1. Chargement et pr√©paration des donn√©es : Le fichier CSV est charg√© et les donn√©es sont nettoy√©es et pr√©par√©es pour l'analyse.\n",
        "2. Calcul de la moyenne glissante : Une requ√™te SQL utilisant DuckDB est ex√©cut√©e pour calculer la moyenne glissante sur 30 jours de la temp√©rature moyenne journali√®re.\n",
        "3. D√©termination des anomalies : Les anomalies thermiques sont d√©termin√©es en comparant les temp√©ratures journali√®res avec la moyenne glissante sur 30 jours.\n",
        "4. Lissage des anomalies : Les anomalies sont ensuite liss√©es sur 7 jours pour r√©duire le bruit et am√©liorer la visualisation.\n",
        "5. Visualisation : Les r√©sultats sont visualis√©s √† l'aide de Plotly, avec des graphiques montrant les temp√©ratures journali√®res, la moyenne glissante, les anomalies, et les anomalies liss√©es.\n",
        "\n",
        "Les donn√©es sont analys√©es √† l'aide de requ√™tes SQL dans DuckDB, puis visualis√©es avec Plotly pour offrir une repr√©sentation claire et interactive des r√©sultats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation et imports\n",
        "import duckdb as ddb\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü¶Ü Chargement du dataset avec Duckdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fonction de chargement compl√®te (bas√©e sur load_file_from_url_lite)\n",
        "def load_file_from_url_lite(url_dataset=\"\", loader=\"read_csv_auto\", options=\"\", nom_table=\"loaded_dataset\", safe_mode=False):\n",
        "    ddb.execute(\"install spatial\")\n",
        "    ddb.execute(\"load spatial\")\n",
        "    ddb.execute(\"INSTALL h3 FROM community\")\n",
        "    ddb.execute(\"LOAD h3\")\n",
        "    ddb.execute(\"install webbed from community;\")\n",
        "    ddb.execute(\"load webbed\")\n",
        "    ddb.execute(\"set force_download=True\")\n",
        "    ddb.execute(f\"drop table if exists {nom_table}\")   \n",
        "    \n",
        "    # D√©tection automatique du type de fichier\n",
        "    if 'csv' in url_dataset: \n",
        "        loader = \"read_csv_auto\"\n",
        "    elif 'tsv' in url_dataset: \n",
        "        loader = \"read_csv_auto\"\n",
        "    elif 'txt' in url_dataset: \n",
        "        loader = \"read_csv_auto\"\n",
        "    elif 'parquet' in url_dataset: \n",
        "        loader = \"read_parquet\"\n",
        "    elif 'json' in url_dataset: \n",
        "        loader = \"read_json_auto\"\n",
        "    elif 'xls' in url_dataset or 'xlsx' in url_dataset: \n",
        "        loader = \"st_read\"\n",
        "    elif 'shp' in url_dataset: \n",
        "        loader = \"st_read\"\n",
        "    elif 'geojson' in url_dataset: \n",
        "        loader = \"st_read\"\n",
        "    elif 'xml' in url_dataset: \n",
        "        loader = \"read_xml\"\n",
        "    elif 'html' in url_dataset: \n",
        "        loader = \"read_html\"\n",
        "    else: \n",
        "        raise ValueError(f\"Type de fichier non support√© pour {url_dataset}\")\n",
        "    \n",
        "    if options==\"\": \n",
        "        options = \"\" \n",
        "    if 'csv' in url_dataset and safe_mode==True: \n",
        "        options = \", all_varchar=1\" \n",
        "    if nom_table==\"\": \n",
        "        nom_table = \"loaded_dataset\"\n",
        "    \n",
        "    try:\n",
        "        status = ddb.sql(f\"\"\"\n",
        "            create or replace table {nom_table} as select *\n",
        "            from\n",
        "            {loader}(\"{url_dataset}\" {options})\n",
        "        \"\"\")\n",
        "        return status\n",
        "    except Exception as e:\n",
        "        return f\"Erreur au chargement du fichier : {str(e)}\"\n",
        "\n",
        "def run_query(sql):\n",
        "    return ddb.sql(sql.replace(\"`\",\" \")).to_df()\n",
        "\n",
        "# Chargement des donn√©es\n",
        "load_file_from_url_lite(\"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/QUOT/Q_95_latest-2024-2025_RR-T-Vent.csv.gz\", safe_mode=True)\n",
        "print(\"‚úÖ Donn√©es charg√©es avec succ√®s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Analyse SQL\n",
        "\n",
        "Cette requ√™te utilise des techniques SQL pour extraire et transformer les donn√©es de mani√®re efficace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ex√©cution de la requ√™te\n",
        "df = run_query(\"\"\" WITH \n",
        "daily_avg AS (\n",
        "  SELECT \n",
        "    CAST(SUBSTRING(\"AAAAMMJJ\", 1, 4) AS INTEGER) AS year,\n",
        "    CAST(SUBSTRING(\"AAAAMMJJ\", 5, 2) AS INTEGER) AS month,\n",
        "    CAST(SUBSTRING(\"AAAAMMJJ\", 7, 2) AS INTEGER) AS day,\n",
        "    CAST(\"TM\" AS DOUBLE) AS \"TM\",\n",
        "    \"AAAAMMJJ\"\n",
        "  FROM \n",
        "    loaded_dataset\n",
        "),\n",
        "avg_tm AS (\n",
        "  SELECT \n",
        "    \"AAAAMMJJ\",\n",
        "    AVG(\"TM\") AS avg_tm_day\n",
        "  FROM \n",
        "    daily_avg\n",
        "  GROUP BY \n",
        "    \"AAAAMMJJ\"\n",
        "),\n",
        "moving_avg AS (\n",
        "  SELECT \n",
        "    \"AAAAMMJJ\",\n",
        "    avg_tm_day,\n",
        "    AVG(avg_tm_day) OVER (ORDER BY \"AAAAMMJJ\" ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) AS moving_avg_30_days\n",
        "  FROM \n",
        "    avg_tm\n",
        "),\n",
        "anomalies AS (\n",
        "  SELECT \n",
        "    \"AAAAMMJJ\",\n",
        "    avg_tm_day,\n",
        "    moving_avg_30_days,\n",
        "    avg_tm_day - moving_avg_30_days AS anomaly\n",
        "  FROM \n",
        "    moving_avg\n",
        ")\n",
        "SELECT \n",
        "  \"AAAAMMJJ\",\n",
        "  avg_tm_day,\n",
        "  moving_avg_30_days,\n",
        "  anomaly,\n",
        "  AVG(anomaly) OVER (ORDER BY \"AAAAMMJJ\" ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING) AS smoothed_anomaly_7_days\n",
        "FROM \n",
        "  anomalies\n",
        "ORDER BY \n",
        "  \"AAAAMMJJ\" \"\"\")\n",
        "print(f\"R√©sultats : {len(df)} lignes\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Visualisation\n",
        "\n",
        "La biblioth√®que principale utilis√©e est Plotly, qui permet de cr√©er des visualisations interactives et personnalis√©es. Cette technologie est adapt√©e pour repr√©senter les temp√©ratures journali√®res et les anomalies thermiques car elle permet de superposer plusieurs trac√©s et d'afficher des informations d√©taill√©es au survol. Cela facilite l'analyse et la compr√©hension des donn√©es."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import duckdb as ddb\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "df['AAAAMMJJ'] = pd.to_datetime(df['AAAAMMJJ'], format='%Y%m%d')\n",
        "\n",
        "dataviz = make_subplots(\n",
        "    rows=2, cols=1,\n",
        "    shared_xaxes=True,\n",
        "    vertical_spacing=0.03,\n",
        "    row_heights=[0.7, 0.3]\n",
        ")\n",
        "\n",
        "temp = go.Scatter(\n",
        "    x=df['AAAAMMJJ'], y=df['avg_tm_day'], mode='lines',\n",
        "    name='Temp√©rature journali√®re', line=dict(color='#1f77b4', width=0.8),\n",
        "    hovertemplate='T¬∞ : %{y:.2f}¬∞C<br>%{x|%Y-%m-%d}<extra></extra>'\n",
        ")\n",
        "dataviz.add_trace(temp, row=1, col=1)\n",
        "\n",
        "moy = go.Scatter(\n",
        "    x=df['AAAAMMJJ'], y=df['moving_avg_30_days'], mode='lines',\n",
        "    name='Moyenne glissante 30 jours', line=dict(color='#ff7f0e', width=2),\n",
        "    hovertemplate='Moyenne : %{y:.2f}¬∞C<br>%{x|%Y-%m-%d}<extra></extra>'\n",
        ")\n",
        "dataviz.add_trace(moy, row=1, col=1)\n",
        "\n",
        "anom = go.Scatter(\n",
        "    x=df['AAAAMMJJ'], y=df['anomaly'], mode='markers',\n",
        "    name='Anomalie', marker=dict(color='rgba(31,119,180,0.4)', size=4),\n",
        "    hovertemplate='Anom. : %{y:.2f}¬∞C<br>%{x|%Y-%m-%d}<extra></extra>'\n",
        ")\n",
        "dataviz.add_trace(anom, row=2, col=1)\n",
        "\n",
        "anom_lisse = go.Scatter(\n",
        "    x=df['AAAAMMJJ'], y=df['smoothed_anomaly_7_days'], mode='lines',\n",
        "    name='Anomalie liss√©e', line=dict(color='crimson', width=2),\n",
        "    hovertemplate='Anom. lis. : %{y:.2f}¬∞C<br>%{x|%Y-%m-%d}<extra></extra>'\n",
        ")\n",
        "dataviz.add_trace(anom_lisse, row=2, col=1)\n",
        "\n",
        "mask = abs(df['anomaly']) > 5\n",
        "points = df[mask]\n",
        "if not points.empty:\n",
        "    dataviz.add_trace(go.Scatter(\n",
        "        x=points['AAAAMMJJ'], y=points['avg_tm_day'], mode='markers',\n",
        "        marker=dict(color='#d62728', size=10, symbol='star'),\n",
        "        name='Anomalie > 5¬∞C',\n",
        "        hovertemplate='T¬∞ : %{y:.2f}¬∞C<br>Anomalie : %{customdata:.2f}¬∞C<br>%{x|%Y-%m-%d}<extra></extra>',\n",
        "        customdata=points['anomaly']\n",
        "    ), row=1, col=1)\n",
        "\n",
        "dataviz.update_layout(\n",
        "    height=700,\n",
        "    margin=dict(l=40, r=20, t=60, b=40),\n",
        "    hovermode='x unified',\n",
        "    font=dict(family='Source Sans Pro', size=12),\n",
        "    title=dict(text=\"Temp√©ratures journali√®res &anomalies thermiques\", x=0.05),\n",
        "    legend=dict(orientation=\"h\", y=-0.05, x=0, xanchor='left'),\n",
        "    xaxis2=dict(title=\"Date\", title_standoff=10),\n",
        "    yaxis=dict(title=\"Temp√©rature (¬∞C)\"),\n",
        "    yaxis2=dict(title=\"Anomalie (¬∞C)\")\n",
        ")\n",
        "dataviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "*Made with ‚ù§Ô∏è and with [duckit.fr](https://duckit.fr) - [Ali Hmaou](https://www.linkedin.com/in/ali-hmaou-6b7b73146/)*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}