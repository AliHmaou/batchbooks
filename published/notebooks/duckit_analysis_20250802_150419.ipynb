{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyse des temp√©ratures maximales journali√®res : une plong√©e dans les donn√©es m√©t√©orologiques fran√ßaises\n",
        "\n",
        "\n",
        "L'analyse pr√©sent√©e dans ce notebook s'appuie sur un jeu de donn√©es m√©t√©orologiques quotidiennes pour diverses stations en France, disponible √† l'adresse suivante : https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/QUOT/Q_95_latest-2024-2025_RR-T-Vent.csv.gz. Ce fichier contient des informations pr√©cieuses sur les pr√©cipitations, les temp√©ratures, les vents, et d'autres param√®tres m√©t√©orologiques. L'objectif principal de cette analyse est de pr√©parer les donn√©es pour examiner les temp√©ratures maximales journali√®res, calculer la moyenne glissante sur 30 jours, et identifier les anomalies par comparaison avec cette moyenne glissante, puis de lisser ces anomalies sur 7 jours.\n",
        "\n",
        "\n",
        "L'√©tude des temp√©ratures maximales journali√®res et de leurs anomalies est cruciale pour comprendre les mod√®les climatiques et m√©t√©orologiques en France. En analysant ces donn√©es, nous pouvons mettre en lumi√®re les tendances et les variations significatives dans les temp√©ratures, contribuant ainsi √† une meilleure compr√©hension du climat et √† une prise de d√©cision √©clair√©e dans divers domaines tels que l'agriculture, l'urbanisme et la gestion des ressources.\n",
        "\n",
        "\n",
        "## M√©thodologie\n",
        "\n",
        "La m√©thodologie employ√©e pour cette analyse implique plusieurs √©tapes cl√©s. Tout d'abord, les donn√©es sont charg√©es et pr√©par√©es √† l'aide de requ√™tes SQL dans DuckDB, permettant un traitement efficace et rapide des grandes quantit√©s de donn√©es. Les temp√©ratures moyennes journali√®res sont calcul√©es et utilis√©es pour d√©terminer la moyenne glissante sur 30 jours. Ensuite, les anomalies sont identifi√©es en comparant les temp√©ratures journali√®res avec cette moyenne glissante. Enfin, ces anomalies sont liss√©es sur 7 jours pour att√©nuer les fluctuations quotidiennes et mettre en √©vidence les tendances sous-jacentes. Les r√©sultats sont ensuite visualis√©s √† l'aide de Plotly pour une repr√©sentation graphique claire et interactive des donn√©es et des analyses effectu√©es."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation et imports\n",
        "import duckdb as ddb\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü¶Ü Chargement du dataset avec Duckdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fonction de chargement compl√®te (bas√©e sur load_file_from_url_lite)\n",
        "def load_file_from_url_lite(url_dataset=\"\", loader=\"read_csv_auto\", options=\"\", nom_table=\"loaded_dataset\", safe_mode=False):\n",
        "    ddb.execute(\"install spatial\")\n",
        "    ddb.execute(\"load spatial\")\n",
        "    ddb.execute(\"INSTALL h3 FROM community\")\n",
        "    ddb.execute(\"LOAD h3\")\n",
        "    ddb.execute(\"install webbed from community;\")\n",
        "    ddb.execute(\"load webbed\")\n",
        "    ddb.execute(\"set force_download=True\")\n",
        "    ddb.execute(f\"drop table if exists {nom_table}\")   \n",
        "    \n",
        "    # D√©tection automatique du type de fichier\n",
        "    if 'csv' in url_dataset: \n",
        "        loader = \"read_csv_auto\"\n",
        "    elif 'tsv' in url_dataset: \n",
        "        loader = \"read_csv_auto\"\n",
        "    elif 'txt' in url_dataset: \n",
        "        loader = \"read_csv_auto\"\n",
        "    elif 'parquet' in url_dataset: \n",
        "        loader = \"read_parquet\"\n",
        "    elif 'json' in url_dataset: \n",
        "        loader = \"read_json_auto\"\n",
        "    elif 'xls' in url_dataset or 'xlsx' in url_dataset: \n",
        "        loader = \"st_read\"\n",
        "    elif 'shp' in url_dataset: \n",
        "        loader = \"st_read\"\n",
        "    elif 'geojson' in url_dataset: \n",
        "        loader = \"st_read\"\n",
        "    elif 'xml' in url_dataset: \n",
        "        loader = \"read_xml\"\n",
        "    elif 'html' in url_dataset: \n",
        "        loader = \"read_html\"\n",
        "    else: \n",
        "        raise ValueError(f\"Type de fichier non support√© pour {url_dataset}\")\n",
        "    \n",
        "    if options==\"\": \n",
        "        options = \"\" \n",
        "    if 'csv' in url_dataset and safe_mode==True: \n",
        "        options = \", all_varchar=1\" \n",
        "    if nom_table==\"\": \n",
        "        nom_table = \"loaded_dataset\"\n",
        "    \n",
        "    try:\n",
        "        status = ddb.sql(f\"\"\"\n",
        "            create or replace table {nom_table} as select *\n",
        "            from\n",
        "            {loader}(\"{url_dataset}\" {options})\n",
        "        \"\"\")\n",
        "        return status\n",
        "    except Exception as e:\n",
        "        return f\"Erreur au chargement du fichier : {str(e)}\"\n",
        "\n",
        "def run_query(sql):\n",
        "    return ddb.sql(sql.replace(\"`\",\" \")).to_df()\n",
        "\n",
        "# Chargement des donn√©es\n",
        "load_file_from_url_lite(\"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/QUOT/Q_95_latest-2024-2025_RR-T-Vent.csv.gz\", safe_mode=True)\n",
        "print(\"‚úÖ Donn√©es charg√©es avec succ√®s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Analyse SQL\n",
        "\n",
        "Cette requ√™te utilise des techniques SQL pour extraire et transformer les donn√©es de mani√®re efficace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ex√©cution de la requ√™te\n",
        "df = run_query(\"\"\" WITH \n",
        "daily_avg AS (\n",
        "  SELECT \n",
        "    CAST(SUBSTRING(\"AAAAMMJJ\", 1, 4) AS INTEGER) AS year,\n",
        "    CAST(SUBSTRING(\"AAAAMMJJ\", 5, 2) AS INTEGER) AS month,\n",
        "    CAST(SUBSTRING(\"AAAAMMJJ\", 7, 2) AS INTEGER) AS day,\n",
        "    CAST(\"TM\" AS DOUBLE) AS \"TM\",\n",
        "    \"AAAAMMJJ\"\n",
        "  FROM \n",
        "    loaded_dataset\n",
        "),\n",
        "avg_tm AS (\n",
        "  SELECT \n",
        "    \"AAAAMMJJ\",\n",
        "    AVG(\"TM\") AS avg_tm_day\n",
        "  FROM \n",
        "    daily_avg\n",
        "  GROUP BY \n",
        "    \"AAAAMMJJ\"\n",
        "),\n",
        "moving_avg AS (\n",
        "  SELECT \n",
        "    \"AAAAMMJJ\",\n",
        "    avg_tm_day,\n",
        "    AVG(avg_tm_day) OVER (ORDER BY \"AAAAMMJJ\" ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) AS moving_avg_30_days\n",
        "  FROM \n",
        "    avg_tm\n",
        "),\n",
        "anomalies AS (\n",
        "  SELECT \n",
        "    \"AAAAMMJJ\",\n",
        "    avg_tm_day,\n",
        "    moving_avg_30_days,\n",
        "    avg_tm_day - moving_avg_30_days AS anomaly\n",
        "  FROM \n",
        "    moving_avg\n",
        ")\n",
        "SELECT \n",
        "  \"AAAAMMJJ\",\n",
        "  avg_tm_day,\n",
        "  moving_avg_30_days,\n",
        "  anomaly,\n",
        "  AVG(anomaly) OVER (ORDER BY \"AAAAMMJJ\" ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING) AS smoothed_anomaly_7_days\n",
        "FROM \n",
        "  anomalies\n",
        "ORDER BY \n",
        "  \"AAAAMMJJ\" \"\"\")\n",
        "print(f\"R√©sultats : {len(df)} lignes\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Visualisation\n",
        "\n",
        "La biblioth√®que principale utilis√©e est Plotly, qui est id√©ale pour cr√©er des visualisations interactives et personnalisables. Plotly est adapt√©e pour repr√©senter des donn√©es temporelles complexes avec plusieurs courbes et des info-bulles d√©taill√©es. Elle permet une repr√©sentation claire et dynamique des donn√©es de temp√©rature journali√®re et de leurs anomalies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import duckdb as ddb\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "df['AAAAMMJJ'] = pd.to_datetime(df['AAAAMMJJ'], format='%Y%m%d')\n",
        "\n",
        "dataviz = go.Figure()\n",
        "\n",
        "dataviz.add_trace(go.Scatter(x=df['AAAAMMJJ'], y=df['avg_tm_day'],\n",
        "                             mode='lines',\n",
        "                             name='Temp√©rature journali√®re',\n",
        "                             line=dict(color='lightgrey'), hovertemplate='%{y:.2f}¬∞C<br>%{x|%Y-%m-%d}<extra></extra>'))\n",
        "\n",
        "dataviz.add_trace(go.Scatter(x=df['AAAAMMJJ'], y=df['moving_avg_30_days'],\n",
        "                             mode='lines',\n",
        "                             name='Moyenne glissante 30 jours',\n",
        "                             line=dict(color='red'), hovertemplate='%{y:.2f}¬∞C<br>%{x|%Y-%m-%d}<extra></extra>'))\n",
        "\n",
        "dataviz.add_trace(go.Scatter(x=df['AAAAMMJJ'], y=df['smoothed_anomaly_7_days'],\n",
        "                             mode='lines',\n",
        "                             name='Anomalie liss√©e 7 jours',\n",
        "                             line=dict(color='teal', dash='dot'), hovertemplate='%{y:.2f}¬∞C<br>%{x|%Y-%m-%d}<extra></extra>'))\n",
        "\n",
        "dataviz.update_layout(\n",
        "    height=500,\n",
        "    margin=dict(l=20, r=20, t=60, b=20),\n",
        "    hovermode='x unified',\n",
        "    font=dict(family='Source Sans Pro', size=12),\n",
        "    title=dict(text=\"Analyse des temp√©ratures maximales journali√®res et anomalies\", x=0.5),\n",
        "    xaxis_title=\"Date\",\n",
        "    yaxis_title=\"Temp√©rature (¬∞C)\",\n",
        "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
        ")\n",
        "dataviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "*Made with ‚ù§Ô∏è and with [duckit.fr](https://duckit.fr) - [Ali Hmaou](https://www.linkedin.com/in/ali-hmaou-6b7b73146/)*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}